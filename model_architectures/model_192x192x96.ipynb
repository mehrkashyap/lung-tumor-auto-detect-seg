{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D U-Net\n",
    "---\n",
    "**Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import montage\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, LeakyReLU\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv3D, Conv3DTranspose\n",
    "from keras.layers.pooling import MaxPooling3D, GlobalMaxPool3D\n",
    "MaxPool3D = MaxPooling3D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import Sequence\n",
    "from keras import backend as K\n",
    "\n",
    "from instancenormalization import *\n",
    "\n",
    "from batchgenerators.dataloading.data_loader import SlimDataLoaderBase\n",
    "from batchgenerators.transforms.spatial_transforms import MirrorTransform, SpatialTransform\n",
    "from batchgenerators.transforms.noise_transforms import GaussianNoiseTransform, GaussianBlurTransform\n",
    "from batchgenerators.transforms.color_transforms import ContrastAugmentationTransform, BrightnessMultiplicativeTransform, GammaTransform\n",
    "from batchgenerators.transforms.resample_transforms import SimulateLowResolutionTransform\n",
    "from batchgenerators.transforms.abstract_transforms import Compose\n",
    "from batchgenerators.dataloading import SingleThreadedAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d_block(input_tensor, n_filters, kernel_size=3, instancenorm=True, leakyrelu=True):\n",
    "    \n",
    "    x = Conv3D(filters=n_filters, kernel_size=kernel_size, kernel_initializer=\"he_normal\", padding=\"same\")(input_tensor)\n",
    "    if instancenorm:\n",
    "        x = InstanceNormalization()(x)\n",
    "    else:\n",
    "        x = BatchNormalization()(x)\n",
    "    if leakyrelu:\n",
    "        x = Activation(LeakyReLU(alpha=0.01))(x)\n",
    "    else:\n",
    "        x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv3D(filters=n_filters, kernel_size=kernel_size, kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "    if instancenorm:\n",
    "        x = InstanceNormalization()(x)\n",
    "    else:\n",
    "        x = BatchNormalization()(x)\n",
    "    if leakyrelu:\n",
    "        x = Activation(LeakyReLU(alpha=0.01))(x)\n",
    "    else:\n",
    "        x = Activation('relu')(x)\n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_vol, n_filters=32, kernel_size=3, instancenorm=True, leakyrelu=True):\n",
    "    c1 = conv3d_block(input_vol, n_filters=n_filters*(2**0), kernel_size=kernel_size, instancenorm=instancenorm, leakyrelu=leakyrelu)\n",
    "    p1 = MaxPool3D((2,2,2))(c1)\n",
    "    \n",
    "    c2 = conv3d_block(p1, n_filters=n_filters*(2**1), kernel_size=kernel_size, instancenorm=instancenorm, leakyrelu=leakyrelu)\n",
    "    p2 = MaxPool3D((2,2,2))(c2)\n",
    "    \n",
    "    c3 = conv3d_block(p2, n_filters=n_filters*(2**2), kernel_size=kernel_size, instancenorm=instancenorm, leakyrelu=leakyrelu)\n",
    "    p3 = MaxPool3D((2,2,2))(c3)\n",
    "    \n",
    "    c4 = conv3d_block(p3, n_filters=n_filters*(2**3), kernel_size=kernel_size, instancenorm=instancenorm, leakyrelu=leakyrelu)\n",
    "    p4 = MaxPool3D((2,2,2))(c4)\n",
    "    \n",
    "    c5 = conv3d_block(p4, n_filters=n_filters*(2**4), kernel_size=kernel_size, instancenorm=instancenorm, leakyrelu=leakyrelu)\n",
    "    p5 = MaxPool3D((2,2,1))(c5)\n",
    "\n",
    "    c6 = conv3d_block(p5, n_filters=n_filters*(2**5), kernel_size=kernel_size, instancenorm=instancenorm, leakyrelu=leakyrelu)\n",
    "    \n",
    "    u7 = Conv3DTranspose(filters=n_filters*(2**4),kernel_size=kernel_size,strides=(2,2,1), padding=\"same\")(c6)\n",
    "    u7 = concatenate([u7,c5])\n",
    "    c7 = conv3d_block(u7,n_filters=n_filters*(2**4))\n",
    "    \n",
    "    u8 = Conv3DTranspose(filters=n_filters*(2**3),kernel_size=kernel_size,strides=(2,2,2), padding=\"same\")(c7)\n",
    "    u8 = concatenate([u8,c4])\n",
    "    c8 = conv3d_block(u8,n_filters=n_filters*(2**3))\n",
    "\n",
    "    u9 = Conv3DTranspose(filters=n_filters*(2**2),kernel_size=kernel_size,strides=(2,2,2), padding=\"same\")(c8)\n",
    "    u9 = concatenate([u9,c3])\n",
    "    c9 = conv3d_block(u9,n_filters=n_filters*(2**2))\n",
    "    \n",
    "    u10 = Conv3DTranspose(filters=n_filters*(2**1),kernel_size=kernel_size,strides=(2,2,2), padding=\"same\")(c9)\n",
    "    u10 = concatenate([u10,c2])\n",
    "    c10 = conv3d_block(u10,n_filters=n_filters*(2**1))\n",
    "    \n",
    "    u11 = Conv3DTranspose(filters=n_filters*(2**0),kernel_size=kernel_size,strides=(2,2,2), padding=\"same\")(c10)\n",
    "    u11 = concatenate([u11,c1])\n",
    "    c11 = conv3d_block(u11,n_filters=n_filters*(2**0))\n",
    "    \n",
    "    outputs = Conv3D(1,(1,1,1),activation=\"sigmoid\")(c11)\n",
    "    model = Model(inputs = [input_vol], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, ids, ct_path, mask_path, patch_size, batch_size=2, min_max_norm=False, seed=2020, augment=True, validation=False, shuffle=True):\n",
    "        \n",
    "        self.ids = ids\n",
    "        self.ct_path = ct_path\n",
    "        self.mask_path = mask_path\n",
    "        self.batch_size = batch_size\n",
    "        self.min_max_norm = min_max_norm\n",
    "        self.shuffle = shuffle\n",
    "        self.patch_size = patch_size\n",
    "        self.seed = seed\n",
    "        self.augment = augment\n",
    "        self.validation = validation\n",
    "        if self.augment == True:\n",
    "            self.transforms = []\n",
    "            self.spatial_transforms = SpatialTransform(self.patch_size, \n",
    "                                          [i // 2 for i in self.patch_size],\n",
    "                                          do_elastic_deform=False,\n",
    "                                          do_rotation=True, \n",
    "                                          angle_x=(-15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
    "                                          angle_y=(-15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
    "                                          angle_z=(-15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
    "                                          do_scale=True,\n",
    "                                          scale=(0.7,1.4),\n",
    "                                          border_mode_data='constant', border_cval_data=0,\n",
    "                                          border_mode_seg='constant', border_cval_seg=0,\n",
    "                                          order_seg=1, order_data=3,\n",
    "                                          p_el_per_sample=0, p_rot_per_sample=0.2, p_scale_per_sample=0.2)\n",
    "\n",
    "            self.transforms.append(self.spatial_transforms)\n",
    "            self.transforms.append(GaussianNoiseTransform(noise_variance=(0.0, 0.1), p_per_sample=0.15))\n",
    "            self.transforms.append(GaussianBlurTransform(blur_sigma=(0.5, 1.5), p_per_sample=0.1))\n",
    "            self.transforms.append(BrightnessMultiplicativeTransform((0.7, 1.3), per_channel=True, p_per_sample=0.15))\n",
    "            self.transforms.append(ContrastAugmentationTransform((0.65, 1.5), preserve_range=True, p_per_sample=0.15))\n",
    "            self.transforms.append(SimulateLowResolutionTransform((1,2), order_downsample=0, order_upsample=3, p_per_sample=0.25))\n",
    "            self.transforms.append(GammaTransform(gamma_range=(0.7, 1.5), invert_image=False, p_per_sample=0.15))\n",
    "            self.transforms.append(GammaTransform(gamma_range=(0.7, 0.71), invert_image=True, p_per_sample=1))\n",
    "            self.transforms.append(MirrorTransform(axes=(0,1,2)))\n",
    "\n",
    "            self.transforms = Compose(self.transforms)\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        'Number of batches per epoch'\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        'Generates one batch of data'\n",
    "        indexes = self.indexes[index:index+1]\n",
    "        ids_batch = [self.ids[k] for k in indexes] # There should just be one id in ids_batch\n",
    "    \n",
    "        X, y = self.__data_generation(ids_batch)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ids))\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "        if self.validation == True:\n",
    "            random.seed(self.seed) # To ensure validation set is the same for each epoch (though will have to review this idea b/c may just be good on the subset of patches that are randomly selected)\n",
    "        \n",
    "    def __data_generation(self, ids_batch):\n",
    "        \n",
    "        'Generates data containing batch samples'\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for _id in ids_batch: # There should just be one _id in ids_batch\n",
    "            \n",
    "            _ct_path = os.path.join(self.ct_path, _id)\n",
    "            _mask_path = os.path.join(self.mask_path, _id)\n",
    "\n",
    "            _mask = load(_mask_path)['arr_0']\n",
    "            _ct_arr = load(_ct_path)['arr_0']\n",
    "            \n",
    "            # Can't be too careful\n",
    "            if (_ct_arr.shape != _mask.shape):\n",
    "                \n",
    "                raise Exception(str(_ct_arr.shape) + \" \" + str(_mask.shape) + \" \" + str(_ct_path))\n",
    "            \n",
    "            # Pad to patch size if CT shape < patch size in any dimension\n",
    "            if (_ct_arr.shape[0] < self.patch_size[0]) or (_ct_arr.shape[1] < self.patch_size[1]) or (_ct_arr.shape[2] < self.patch_size[2]):\n",
    "                \n",
    "                _padded_ct_arr = np.zeros((max(self.patch_size[0],_ct_arr.shape[0]),\n",
    "                                       max(self.patch_size[1],_ct_arr.shape[1]),\n",
    "                                       max(self.patch_size[2],_ct_arr.shape[2])))\n",
    "                \n",
    "                _padded_mask = np.zeros((max(self.patch_size[0],_ct_arr.shape[0]),\n",
    "                                       max(self.patch_size[1],_ct_arr.shape[1]),\n",
    "                                       max(self.patch_size[2],_ct_arr.shape[2])))\n",
    "                \n",
    "                pad_x = max(self.patch_size[0] - _ct_arr.shape[0],0)\n",
    "                pad_y = max(self.patch_size[1] - _ct_arr.shape[1],0)\n",
    "                pad_z = max(self.patch_size[2] - _ct_arr.shape[2],0)\n",
    "                \n",
    "                \n",
    "                _padded_ct_arr[pad_x//2:pad_x//2+_ct_arr.shape[0],\n",
    "                            pad_y//2:pad_y//2+_ct_arr.shape[1],\n",
    "                            pad_z//2:pad_z//2+_ct_arr.shape[2]] = _ct_arr\n",
    "                \n",
    "                _padded_mask[pad_x//2:pad_x//2+_ct_arr.shape[0],\n",
    "                            pad_y//2:pad_y//2+_ct_arr.shape[1],\n",
    "                            pad_z//2:pad_z//2+_ct_arr.shape[2]] = _mask\n",
    "                \n",
    "                _ct_arr = _padded_ct_arr\n",
    "                _mask = _padded_mask\n",
    "            \n",
    "\n",
    "            # Min/max normalization\n",
    "            if self.min_max_norm == True:\n",
    "                _ct_arr = np.clip(_ct_arr, 0, 1500)\n",
    "                _ct_arr /= 1500\n",
    "            \n",
    "            # Z Score Normalization (DEFAULT)\n",
    "            else:\n",
    "                _ct_arr = _ct_arr - np.mean(_ct_arr)\n",
    "                _ct_arr /= np.std(_ct_arr)\n",
    "            \n",
    "            # pick 10 patches, make sure all contain tumor (for non-iSABR pts)\n",
    "            for patch in range(self.batch_size):\n",
    "                \n",
    "                patch_contains_tumor = False\n",
    "\n",
    "                if _id not in self.isabr and patch_contains_tumor == False and (_mask.max() == 1):\n",
    "                    while patch_contains_tumor == False:\n",
    "                        r_x = random.choice(range(_ct_arr.shape[0]-self.patch_size[0]+1))\n",
    "                        r_y = random.choice(range(_ct_arr.shape[1]-self.patch_size[1]+1))\n",
    "                        r_z = random.choice(range(_ct_arr.shape[2]-self.patch_size[2]+1))\n",
    "\n",
    "                        _ct_patch = _ct_arr[r_x:r_x+self.patch_size[0],\n",
    "                                            r_y:r_y+self.patch_size[1],\n",
    "                                            r_z:r_z+self.patch_size[2]]\n",
    "\n",
    "                        _mask_patch = _mask[r_x:r_x+self.patch_size[0],\n",
    "                                            r_y:r_y+self.patch_size[1],\n",
    "                                            r_z:r_z+self.patch_size[2]]\n",
    "\n",
    "                        if _mask_patch.max() == 1:\n",
    "                            patch_contains_tumor = True\n",
    "                else:\n",
    "                    r_x = random.choice(range(_ct_arr.shape[0]-self.patch_size[0]+1))\n",
    "                    r_y = random.choice(range(_ct_arr.shape[1]-self.patch_size[1]+1))\n",
    "                    r_z = random.choice(range(_ct_arr.shape[2]-self.patch_size[2]+1))\n",
    "\n",
    "                    _ct_patch = _ct_arr[r_x:r_x+self.patch_size[0],\n",
    "                                        r_y:r_y+self.patch_size[1],\n",
    "                                        r_z:r_z+self.patch_size[2]]\n",
    "\n",
    "                    _mask_patch = _mask[r_x:r_x+self.patch_size[0],\n",
    "                                        r_y:r_y+self.patch_size[1],\n",
    "                                        r_z:r_z+self.patch_size[2]]\n",
    "                \n",
    "                # Augmentations\n",
    "                if self.augment == True:\n",
    "\n",
    "                    _ct_patch = np.expand_dims(_ct_patch,axis=0)\n",
    "                    _ct_patch = np.expand_dims(_ct_patch,axis=0)\n",
    "\n",
    "                    _mask_patch = np.expand_dims(_mask_patch,axis=0)\n",
    "                    _mask_patch = np.expand_dims(_mask_patch,axis=0)\n",
    "\n",
    "                    _ct_patch = _ct_patch.astype('float32') \n",
    "                    \n",
    "                    data = AugmentationDataLoader({'vol': _ct_patch, 'seg': _mask_patch})\n",
    "                    #multithread_generator = MultiThreadedAugmenter(data, self.transforms,num_processes=1,num_cached_per_queue=1)\n",
    "                    multithread_generator = SingleThreadedAugmenter(data, self.transforms)\n",
    "\n",
    "                    augmented = next(multithread_generator)\n",
    "\n",
    "                    _ct_patch = augmented['data']\n",
    "                    _mask_patch = augmented['seg']\n",
    "\n",
    "\n",
    "                    _ct_patch = _ct_patch[0,0,:,:,:]\n",
    "                    _mask_patch = _mask_patch[0,0,:,:,:]\n",
    "                \n",
    "                X.append(_ct_patch)\n",
    "                y.append(_mask_patch)\n",
    "        \n",
    "        X = np.array(X, dtype = 'float32')\n",
    "        y = np.array(y, dtype = 'float32')\n",
    "        \n",
    "        X = X.reshape(-1,X.shape[1],X.shape[2],X.shape[3],1)\n",
    "        y = y.reshape(-1,y.shape[1],y.shape[2],y.shape[3],1)  \n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "class AugmentationDataLoader(SlimDataLoaderBase):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "\n",
    "        super(AugmentationDataLoader,self).__init__(data, batch_size=1)\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "\n",
    "        vol = self._data['vol']\n",
    "        seg = self._data['seg']\n",
    "\n",
    "        return {'data':vol.astype(np.float32), 'seg':seg}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
