{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "---\n",
    "**Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import CSVLogger\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session =tf.compat.v1.InteractiveSession(config=config)\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select model\n",
    "from model_192x192x96 import *\n",
    "#from model_192x160x80 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings and Model Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_dir = \"\" #path to directory with processed CTs (output of preprocess script)\n",
    "mask_dir = \"\" #path to directory with processed masks (output of preprocess script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"[model_parameters].{epoch:02d}-{val_loss:.2f}.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (192,160,80)\n",
    "augment = True\n",
    "batch_size = 2\n",
    "val_batch_size = 2\n",
    "loss_function = bce_dice\n",
    "n_filters = 32\n",
    "metrics = [dice_coef, 'accuracy']\n",
    "instancenorm = False # will use batch norm if false\n",
    "leakyrelu = True\n",
    "epochs = 200\n",
    "min_max_norm = False\n",
    "csv_logger = CSVLogger('', append=True, separator=';') #name of csv file to log training info\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=30, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss',factor=0.1, patience=10, min_lr=0.000001, verbose=1),\n",
    "    ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True), \n",
    "    csv_logger\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train/Val Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = next(os.walk(mask_dir))[2]\n",
    "random.shuffle(ids)\n",
    "\n",
    "pts = np.unique([i.split(\"_\")[0] for i in ids])\n",
    "val_data_size = round(len(pts)*0.14) # 0.14 gives a val set size of 15% (0.15 not used b/c this gives too many val CTs after accounting for pts with more than one CT)\n",
    "train_pts = pts[val_data_size:]\n",
    "val_pts = pts[:val_data_size]\n",
    "\n",
    "train_ids = [i for i in ids if i.split(\"_\")[0] in train_pts] \n",
    "val_ids = [i for i in ids if i.split(\"_\")[0] in val_pts] \n",
    "\n",
    "val_CT_dir = CT_dir\n",
    "val_mask_dir = mask_dir\n",
    "\n",
    "print(\"Train\", len(train_ids), \"CTs\")\n",
    "print(\"Val:\", len(val_ids), \"CTs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example CT Volume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "gen = DataGenerator(train_ids, CT_dir, mask_dir, patch_size=patch_size, batch_size=batch_size, min_max_norm=min_max_norm, validation=False, augment=augment, seed=seed, shuffle=True)\n",
    "x, y = gen.__getitem__(random.randrange(len(gen)))\n",
    "\n",
    "for i in range(len(x)):\n",
    "    fig, ax = plt.subplots(figsize=(40,40))\n",
    "    ax.imshow(montage(np.moveaxis(x[i,:,:,:,0],-1,0)), cmap=\"gray\")\n",
    "    ax.imshow(montage(np.moveaxis(-y[i,:,:,:,0],-1,0)), cmap=\"flag\", alpha = 0.2)\n",
    "print(x.shape)\n",
    "\n",
    "# If image is red, this is b/c augmentation resulted in ct/patch w/o tumor (e.g. from scaling/rotations) - happens infrequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Compile Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "input_vol = Input((patch_size+(1,)), name='vol')\n",
    "model = unet(input_vol, n_filters=n_filters, instancenorm=instancenorm, leakyrelu=leakyrelu)\n",
    "model.compile(optimizer=Adam(), loss=loss_function, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and val generators\n",
    "train_gen = DataGenerator(train_ids, CT_dir, mask_dir, patch_size=patch_size, batch_size=batch_size, min_max_norm=min_max_norm, seed=seed, validation=False, augment=augment, shuffle=True)\n",
    "val_gen = DataGenerator(val_ids, val_CT_dir, val_mask_dir, patch_size=patch_size, batch_size=val_batch_size, min_max_norm=min_max_norm, seed=seed, validation=True, augment=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = model.fit_generator(train_gen, validation_data=val_gen, callbacks=callbacks, epochs=epochs, verbose=1, use_multiprocessing=True, workers=39, max_queue_size=39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot learning curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
